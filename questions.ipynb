{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 14675,
     "status": "ok",
     "timestamp": 1729102802025,
     "user": {
      "displayName": "Saksham Gupta",
      "userId": "01356306476507643666"
     },
     "user_tz": -330
    },
    "id": "uT2DZEXZjhTy",
    "outputId": "1316ed46-ed2b-4d5e-a66a-749b2379d7ba"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a245b415274f42578dd86794cc46fb89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.pdf', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313007f73d0a4d17be98606ec75dbfdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import io\n",
    "\n",
    "# Step 1: Create a file upload widget\n",
    "upload_widget = widgets.FileUpload(accept='.pdf', multiple=False)\n",
    "output_widget = widgets.Output()  # Widget to display output\n",
    "display(upload_widget, output_widget)\n",
    "\n",
    "# Step 2: Function to extract text from the uploaded PDF\n",
    "def extract_text_from_pdf(pdf_bytes):\n",
    "    reader = PyPDF2.PdfReader(io.BytesIO(pdf_bytes))\n",
    "    text = \"\"\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        text += page.extract_text() + \"\\n\"  # Adding newline for better readability\n",
    "    return text\n",
    "\n",
    "extracted_text = \"\"\n",
    "\n",
    "# Step 3: Callback function to handle the upload\n",
    "def on_upload_change(change):\n",
    "    global extracted_text  # Declare as global to use it later for question generation\n",
    "\n",
    "    # Clear previous output\n",
    "    with output_widget:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        # Get the uploaded file content (in-memory)\n",
    "        uploaded_file = next(iter(upload_widget.value))  # Correct way to get file\n",
    "        pdf_file_content = uploaded_file['content']  # Get the file content in bytes\n",
    "        \n",
    "        # Step 4: Extract text from the PDF bytes\n",
    "        extracted_text = extract_text_from_pdf(pdf_file_content)\n",
    "        \n",
    "        # Step 5: Display the extracted text\n",
    "        print(\"Extracted Text:\")\n",
    "        print(extracted_text)  # Print the extracted text inside the output widget\n",
    "\n",
    "# Step 6: Attach the callback to the upload widget\n",
    "upload_widget.observe(on_upload_change, names='value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nisht\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nisht\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\python 311\\lib\\site-packages (from gensim) (1.25.2)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\python 311\\lib\\site-packages (from gensim) (1.11.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\python 311\\lib\\site-packages (from gensim) (6.4.0)\n",
      "Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.3/24.0 MB 5.2 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 3.1/24.0 MB 6.8 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.7/24.0 MB 7.3 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 6.6/24.0 MB 7.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 8.9/24.0 MB 8.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 11.0/24.0 MB 8.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 12.6/24.0 MB 8.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 13.1/24.0 MB 8.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 15.2/24.0 MB 8.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 16.5/24.0 MB 7.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.1/24.0 MB 7.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 19.9/24.0 MB 7.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.0/24.0 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.0 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 7.8 MB/s eta 0:00:00\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.3.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2kNN8zZ1df5H"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def extract_topics(text, num_topics=5):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text.lower())\n",
    "    filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    dictionary = corpora.Dictionary([filtered_words])\n",
    "    doc_term_matrix = [dictionary.doc2bow(filtered_words)]\n",
    "\n",
    "    lda_model = LdaModel(doc_term_matrix, num_topics=num_topics, id2word=dictionary, passes=50)\n",
    "    topics = lda_model.print_topics(num_words=4)\n",
    "    return topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\python 311\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\python 311\\lib\\site-packages (from tiktoken) (2024.4.16)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\python 311\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python 311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python 311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python 311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python 311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\python 311\\lib\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48215,
     "status": "ok",
     "timestamp": 1729104912462,
     "user": {
      "displayName": "Saksham Gupta",
      "userId": "01356306476507643666"
     },
     "user_tz": -330
    },
    "id": "KUIMBcIHn5Uf",
    "outputId": "bbe95f78-dc6a-48fd-bcf2-9dfc2e539ac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\python 311\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in c:\\python 311\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\python 311\\lib\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python 311\\lib\\site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python 311\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python 311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python 311\\lib\\site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: requests in c:\\python 311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\python 311\\lib\\site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\python 311\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python 311\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python 311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python 311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\python 311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python 311\\lib\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python 311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python 311\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python 311\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c1f04aa15f43e0afd021cb2f92adec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e59e33c04f41f9b1ad9c4420aff421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d93eef62744390b0302f480b1e4cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/15.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ca299fcdff4e2c9b379d61546103ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b961c19e487a45f19d5436ed882ba4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\python 311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2311bfe276fb4c178809a2871846e12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What does Prajna AI Wizzify Your Data do?\n",
      "Question 2: Prajna AI - Confidential & Proprietary HACKATHON PROBLEM STATEMENT What does Prajna AI Wizzify Your Data do?\n",
      "Question 3: Prajna AI - Confidential & Proprietary HACKATHON PROBLEM STATEMENT What does Prajna AI Wizzify Your Data?\n",
      "Question 4: Prajna AI - Confidential & Proprietary HACKATHON PROBLEM STATEMENT Prajna AI Wizzify Your Data\n",
      "Question 5: Prajna AI - Confidential & Proprietary HACKATHON PROBLEM STATEMENT Prajna AI Wizzify Your Data What is the name of the problem?\n",
      "Question 6: What is the goal of the hackathon?\n",
      "Question 7: What is the goal of this hackathon?\n",
      "Question 8: What is the purpose of the hackathon?\n",
      "Question 9: What is the aim of the hackathon?\n",
      "Question 10: What is the goal of the Hackathon?\n",
      "Question 11: What does Citation and Validation do?\n",
      "Question 12: What does Citation and Validation provide?\n",
      "Question 13: What does Citation and Validation do for PDF documents?\n",
      "Question 14: What does Citation and Validation do for PDFs?\n",
      "Question 15: What does Citation and Validation mean?\n",
      "Question 16: What is the criteria for uploading and parsing PDF documents?\n",
      "Question 17: What are the requirements of a PDF Document Ingestion 10 MARKS?\n",
      "Question 18: What are the requirements for a PDF Document Ingestion 10 MARKS?\n",
      "Question 19: What is the criteria for uploading and parsing PDF documents accurate?\n",
      "Question 20: What is the criteria for uploading and parsing PDF documents accurate to?\n",
      "Question 21: What are the criteria for generating embeddings?\n",
      "Question 22: What are the criteria for generating embeddings in a PDF?\n",
      "Question 23: What is the purpose of preserving the structure of a PDF file?\n",
      "Question 24: What are the criteria for creating embeddings?\n",
      "Question 25: What is the purpose of preserving the structure of the content?\n",
      "Question 26: What kind of questions does the Question Suggestion Engine generate?\n",
      "Question 27: What kind of questions does Question Suggestion Engine generate?\n",
      "Question 28: What kind of questions does the Question Suggestion Engine automatically generate?\n",
      "Question 29: What does the Question Suggestion Engine generate?\n",
      "Question 30: What questions does Question Suggestion Engine generate?\n",
      "Question 31: What are some of the criteria for evaluating a PDF?\n",
      "Question 32: What are the criteria for evaluating a PDF's user Querying Interface?\n",
      "Question 33: What is one of the criteria for evaluating a PDF?\n",
      "Question 34: What are the criteria for evaluating a PDF?\n",
      "Question 35: What are the criteria for evaluating a PDF's user-generated questions?\n",
      "Question 36: What is a query structure?\n",
      "Question 37: What is the purpose of a query structure?\n",
      "Question 38: What type of query structures can be used?\n",
      "Question 39: What is the purpose of a citation structure?\n",
      "Question 40: What is the purpose of a citation?\n",
      "Question 41: What is the criteria for deploying the entire system on a cloud platform?\n",
      "Question 42: What is the criteria for deploying a system on a cloud platform?\n",
      "Question 43: What is the criteria for deploying the system on a cloud platform?\n",
      "Question 44: What is the criteria for deploying the entire system on a cloud platform with considd?\n",
      "Question 45: What is the criteria for deploying a system on a cloud platform with considd?\n",
      "Question 46: What is an example of a free API that meets the accuracy requirement?\n",
      "Question 47: What is an example of a free service that meets the accuracy requirement?\n",
      "Question 48: What is an example of a free API that meets the accuracy requirements?\n",
      "Question 49: What is an example of an API that meets the accuracy requirement?\n",
      "Question 50: What is an example of a free service that meets the accuracy requirements?\n",
      "Question 51: What is a customizable user dashboard?\n",
      "Question 52: What is a custom user dashboard?\n",
      "Question 53: What is a customizable user dashboard?\n",
      "Question 54: What is a custom user dashboard that can be customized?\n",
      "Question 55: What is a custom user dashboard for Crea?\n",
      "Question 56: What does Notebook LM by Google offer insights into handling document -based interactions?\n",
      "Question 57: What does Notebook LM by Google offer insights into handling document based interactions?\n",
      "Question 58: What does Notebook LM by Google offer insight into handling document based interactions?\n",
      "Question 59: What does Notebook LM by Google offer insights into handling document interactions?\n",
      "Question 60: What does Notebook LM by Google offer insight into handling document interactions?\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "\n",
    "from transformers import T5Tokenizer, pipeline\n",
    "\n",
    "# Load the fast tokenizer (if available)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"valhalla/t5-base-qg-hl\", use_fast=True)\n",
    "\n",
    "# Initialize the question generation pipeline with a specific model\n",
    "question_generator = pipeline(\"text2text-generation\", model=\"valhalla/t5-base-qg-hl\", num_beams=5)\n",
    "\n",
    "# Function to generate questions from the extracted text\n",
    "def generate_questions(text, num_questions=5, max_length=128):\n",
    "    questions = []\n",
    "\n",
    "    # Tokenize the input text\n",
    "    tokenized_text = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    # Calculate the number of tokens in the input\n",
    "    num_tokens = tokenized_text['input_ids'].shape[1]\n",
    "\n",
    "    # Adjust chunk size based on token limits\n",
    "    chunk_size = 512  # Adjust if necessary based on model limits\n",
    "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # Add a task prefix to guide the model\n",
    "        input_text = f\"generate questions: {chunk}\"\n",
    "\n",
    "        try:\n",
    "            # Generate questions using the question-generation pipeline\n",
    "            generated_questions = question_generator(input_text, max_length=max_length, num_return_sequences=num_questions)\n",
    "\n",
    "            # Collect generated questions\n",
    "            questions.extend([q['generated_text'] for q in generated_questions])  # Access generated_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating questions for chunk: {chunk}\\nError: {str(e)}\")\n",
    "\n",
    "    return questions\n",
    "\n",
    "\n",
    "# Generate questions\n",
    "# extracted_text = \"Your text here.\"  # Replace this with your actual extracted text\n",
    "questions = generate_questions(extracted_text, num_questions=5)\n",
    "\n",
    "# Print the questions\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"Question {i}: {question}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4vYkYx6ntc3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPGsuifJm3Gbwy+0XaYeJIw",
   "provenance": [
    {
     "file_id": "1XMahZF-lFbPiZBjrqkJRjxEBoV-Kk8Jo",
     "timestamp": 1729101331518
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
